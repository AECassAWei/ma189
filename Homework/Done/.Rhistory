1+1
mean(c(1, 2, 3))
1+1
Sys.getlocale()
Sys.setenv
Sys.setenv(LANG = "fr")
x
2 + x
Sys.getlocale()
Sys.setlocale("LC_MESSAGES", "en_US.utf8")
install.packages('tidyverse')
install.package('tidyverse')
install.package('ggplot')
install.packages('ggplot')
install.packages('ggplot2')
version
install.packages("tidyverse")
a = c(1, 2, 3, 4, 5)
a = a * 100
a
(a * 100)
a = (a * 100)
a
x <- as.matrix(baby[1:3,1:2])
x
x <- as.matrix(baby[1:3,1:2])
library(tidyverse)
load(url('https://vulstats.ucsd.edu/labs/201b-2021/01/lab1.RData'))
glimpse(iqdat)
with(iqdat, cor(IQ, Score))
iqdat %>%
ggplot(aes(IQ, Score)) +
geom_point()
obs <- with(iqdat, cor(IQ, Score, method = 'spearman'))
str(iqdat)
glimpse(iqdat)
summary(iqdat)
with(iqdat, cor(IQ, Score))
shuffle1 <- function(d) {
d %>%
mutate(Score = sample(Score, n(), replace = F))
}
shuffle1.fast <- function(d) {
data.frame(IQ = d$IQ,
Score = sample(d$Score, nrow(d), replace = F))
}
system.time(replicate(100, shuffle1(iqdat)))
system.time(replicate(100, shuffle1.fast(iqdat)))
n.shuffle <- 10000
null1 <- replicate(n.shuffle, with(shuffle1.fast(iqdat), cor(IQ, Score, method = 'spearman')))
hist(null1)
2 * (sum(null1 > obs) + 1) / (n.shuffle + 2)
sample1 <- function(d) {
idx <- sample(1:nrow(d), nrow(d), replace = T)
d[idx,]
}
sample1(iqdat)
n.sample <- 10000
sampling1 <- replicate(n.sample, with(sample1(iqdat), cor(IQ, Score, method = 'spearman')))
hist(sampling1)
quantile(sampling1, c(.025, .975))
library(tidyverse)
load(url('https://vulstats.ucsd.edu/labs/201b-2021/01/lab1.RData'))
glimpse(iqdat)
with(iqdat, cor(IQ, Score))
iqdat %>%
ggplot(aes(IQ, Score)) +
geom_point()
obs <- with(iqdat, cor(IQ, Score, method = 'spearman'))
shuffle1 <- function(d) {
d %>%
mutate(Score = sample(Score, n(), replace = F))
}
shuffle1.fast <- function(d) {
data.frame(IQ = d$IQ,
Score = sample(d$Score, nrow(d), replace = F))
}
system.time(replicate(100, shuffle1(iqdat)))
system.time(replicate(100, shuffle1.fast(iqdat)))
n.shuffle <- 10000
null1 <- replicate(n.shuffle, with(shuffle1.fast(iqdat), cor(IQ, Score, method = 'spearman')))
hist(null1)
2 * (sum(null1 > obs) + 1) / (n.shuffle + 2)
sample1 <- function(d) {
idx <- sample(1:nrow(d), nrow(d), replace = T)
d[idx,]
}
sample1(iqdat)
n.sample <- 10000
sampling1 <- replicate(n.sample, with(sample1(iqdat), cor(IQ, Score, method = 'spearman')))
hist(sampling1)
quantile(sampling1, c(.025, .975))
ASL.Lex <- read_csv('http://vulstats.ucsd.edu/data/ASL.Lex.clean.csv')
dat <- ASL.Lex %>%
filter(LexicalClass %in% c('Noun', 'Verb')) %>%
transmute(Gloss = Gloss,
LexicalClass = as.factor(LexicalClass),
Location = as.factor(Location))
dat %>%
count(LexicalClass, Location, .drop = F) %>%
group_by(LexicalClass) %>%
mutate(p = n/sum(n)) %>%
ggplot(aes(x = Location, y = p, fill = LexicalClass)) +
geom_col(position = position_dodge()) +
theme_minimal() +
coord_flip()
entropy <- function(v) {
p <- v / sum(v)
q <- log2(p)*p
q[is.na(q)] <- 0
-sum(q)
}
dat %>%
count(LexicalClass, Location, .drop = F) %>%
group_by(LexicalClass) %>%
summarize(H = entropy(n), .groups = 'drop')
ent.diff <- function(d) {
d %>%
count(LexicalClass, Location, .drop = F) %>%
group_by(LexicalClass) %>%
summarize(H = entropy(n), .groups = 'drop') %>%
summarize(diff = H[LexicalClass == 'Noun'] - H[LexicalClass == 'Verb']) %>%
pull(diff)
}
ent.diff.fast <- function(d) {
tab <- table(d$LexicalClass, d$Location)
entropy(tab['Noun',]) - entropy(tab['Verb',])
}
system.time(replicate(100, ent.diff(dat)))
system.time(replicate(100, ent.diff.fast(dat)))
glimpse(dat)
shuffle2 <- function(d) {
data.frame(Location = d$Location,
LexicalClass = sample(d$LexicalClass, nrow(d), replace = F))
}
shuffle2(dat)
n.shuffle <- 10000
null2 <- replicate(n.shuffle, ent.diff.fast(shuffle2(dat)))
hist(null2)
2 * (sum(null2 > ent.diff.fast(dat)) + 1) / (n.shuffle + 2)
sample2 <- function(d) {
nouns <- d$LexicalClass == 'Noun'
verbs <- !nouns
noun.idx <- sample(which(nouns), sum(nouns), replace = T)
verb.idx <- sample(which(verbs), sum(verbs), replace = T)
d[c(noun.idx, verb.idx),]
}
sample2(dat)
n.sample <- 10000
sampling2 <- replicate(n.sample, ent.diff.fast(sample2(dat)))
hist(sampling2)
quantile(sampling2, c(.025, .975))
tinytex::uninstall_tinytex()
tinytex::install_tinytex()
setwd("E:/UCSD/2020-2021 Senior/WI 21/MATH 189 HW")
# t() caluclates the transpose of the inputted matrix
A = t(X) %*% X
# import dataset
babies <- read.table("Data/babies.dat", sep="")
# head retrieves the first n data points from our dataset
head(babies[,c(1,4,6)])
# tail retrieves the last n data points from our dataset
X = as.matrix(tail(babies[,c(1,4,6)],5))
X
# t() caluclates the transpose of the inputted matrix
A = t(X) %*% X
t(X)
# import dataset
babies <- read.csv("Data/babies.dat", sep="")
# head retrieves the first n data points from our dataset
head(babies[,c(1,4,6)])
# tail retrieves the last n data points from our dataset
X = as.matrix(tail(babies[,c(1,4,6)],5))
X
# t() caluclates the transpose of the inputted matrix
A = t(X) %*% X
A
setwd("E:/UCSD/2020-2021 Senior/WI 21/MATH 189 HW")
